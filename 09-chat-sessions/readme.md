## Managing Chat Sessions with Genkit

Large Language Models (LLMs) are capable of much more than simply generating text, and many users first interact with them through chatbots. This conversational style of prompting is a powerful way to influence the output generated by an AI model, even when users aren't directly chatting with the model. To support this interaction style, Genkit provides interfaces and abstractions for building chat-based LLM applications.

Genkit's chat API is currently in Beta, meaning its APIs may change in minor version releases.

### 1. Chat Session Basics

Genkit simplifies building chatbots by providing `ai.chat()`, which allows the model to leverage previous interactions within a session.

When you send a message to a chat session, Genkit automatically performs several actions behind the scenes:
*   It retrieves any existing chat history from storage.
*   It sends the request to the model, similar to `generate()`, but automatically includes the chat history.
*   It saves the model's response into the chat history.

#### 1.1. Minimal Chatbot Example
To begin, ensure you have Genkit and necessary model plugins (e.g., `@genkit-ai/googleai`) installed and configured.

```typescript
import { genkit } from 'genkit/beta'; // Note: chat API is in beta and imported from genkit/beta
import { googleAI } from '@genkit-ai/googleai';
import { createInterface } from 'node:readline/promises';

const ai = genkit({
  plugins: [ googleAI() ],
  model: googleAI.model('gemini-2.5-flash'),
});

async function main() {
  const chat = ai.chat();
  console.log("You're chatting with Gemini. Ctrl-C to quit. \n");
  const readline = createInterface(process.stdin, process.stdout);
  while (true) {
    const userInput = await readline.question('> ');
    const { text } = await chat.send(userInput);
    console.log(text);
  }
}

main();
```
In this example, the model remembers previous interactions, such as the user's name, demonstrating how context is maintained across messages.
